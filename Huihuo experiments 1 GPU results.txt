(Epoch 1 skipped)
                        Metric	           Mean	         Median	            Std	   1st quartile	      3rd quartile
2 batch 1 worker
                load_batch_mem:	         0.0058	         0.0001	         0.0586	         0.0001	         0.0001
                load_batch_gpu:	         0.0021	         0.0021	         0.0001	          0.002	         0.0021
                   all_compute:	         0.8302	         0.8300	         0.0146	         0.8276	         0.8323
2 batch 0 worker 
                load_batch_mem:	         0.3322	         0.2769	         0.2151	         0.1633	         0.4583
                load_batch_gpu:	         0.0021	         0.0021	            0.0	         0.0021	         0.0021
                   all_compute:	         0.8265	         0.8261	         0.0125	         0.8239	         0.8284
4 batch 1 worker 
                load_batch_mem:	         0.0191	         0.0001	         0.1266	         0.0001	         0.0002
                load_batch_gpu:	         0.0041	         0.0040	         0.0003	          0.004	         0.0041
                   all_compute:	          1.362	         1.3604	         0.0203	         1.3582	         1.3636
4 batch 0 worker 
                load_batch_mem:	         0.6633	         0.6229	         0.3042	         0.4201	         0.8448
                load_batch_gpu:	          0.004	          0.004	         0.0001	          0.004	          0.004
                   all_compute:	         1.3498	         1.3485	         0.0171	         1.3461	         1.3515

Original workload: having 1 worker makes load_batch_mem 1.7% and 2.8% of what it was with 0 worker for batch sizes 2 and 4 resp.

2 batch 1 worker - no step 7 
                load_batch_mem:	         0.0068	         0.0001	         0.0603	         0.0001	         0.0001
                load_batch_gpu:	         0.5697	         0.5766	         0.0566	         0.5744	         0.5786
                   all_compute:	         0.2565	         0.2558	         0.0143	          0.255	         0.2568
                 compute + gpu:                             0.8324
2 batch 0 worker - no step 7      
                load_batch_mem:	          0.339	         0.2825	         0.2178	         0.1672	         0.4713
                load_batch_gpu:	         0.2619	         0.2927	         0.1675	         0.1031	         0.4084
                   all_compute:	         0.2551	         0.2545	         0.0129	         0.2539	         0.2553
                 compute + gpu:                             0.5472
4 batch 1 worker - no step 7 
                load_batch_mem:	         0.0252	         0.0001	         0.1408	         0.0001	         0.0001
                load_batch_gpu:	         0.8988	         0.9235	         0.1383	         0.9216	         0.9258
                   all_compute:	         0.4415	         0.4398	         0.0208	          0.439	         0.4411
                 compute + gpu:                             1.3633
4 batch 0 worker - no step 7 
                load_batch_mem:	         0.6902	         0.6445	         0.3078	         0.4469	         0.8888
                load_batch_gpu:	         0.2800	         0.2732	         0.2243	         0.0306	         0.4755
                   all_compute:	         0.4386	         0.4372	         0.0176	         0.4364	         0.4382
                 compute + gpu:                             0.7104

When step 7 is removed, we see similar reductions in load_batch_mem with a worker. 
However, having 1 worker seems to increase load_batch_gpu 2 to 3x, as well as make it less variable.

Strange: with 1 worker, the sum of the compute and gpu loading is almost exactly the same as before, in the original workload.

It's like the time saved by skipping step 7 is completely offset by increased time to load to gpu.

